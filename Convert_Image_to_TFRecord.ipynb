{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convert Image to TFRecord",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zatihakim/github-demo/blob/master/Convert_Image_to_TFRecord.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xoWvuaF2faF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ab80816-b8be-4e31-a4f8-025314226417"
      },
      "source": [
        "!pip install -q kaggle\n",
        "!wget http://sawat.odellobrien.com/kaggle.json\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d dashhax/vggface21kmasked\n",
        "!unzip -qq vggface21kmasked.zip\n",
        "!mv vggface2sub_train_cropped_masked data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-23 13:25:44--  http://sawat.odellobrien.com/kaggle.json\n",
            "Resolving sawat.odellobrien.com (sawat.odellobrien.com)... 173.82.86.154\n",
            "Connecting to sawat.odellobrien.com (sawat.odellobrien.com)|173.82.86.154|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 63 [application/json]\n",
            "Saving to: ‘kaggle.json’\n",
            "\n",
            "kaggle.json         100%[===================>]      63  --.-KB/s    in 0s      \n",
            "\n",
            "2020-11-23 13:25:44 (8.27 MB/s) - ‘kaggle.json’ saved [63/63]\n",
            "\n",
            "Downloading vggface21kmasked.zip to /content\n",
            "100% 5.52G/5.52G [01:59<00:00, 87.7MB/s]\n",
            "100% 5.52G/5.52G [01:59<00:00, 49.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcVuiK2bpN4H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0c7b43e-6015-48fc-803d-41d4eb03dede"
      },
      "source": [
        "import os, sys, math\n",
        "import numpy as np\n",
        "import random\n",
        "from glob import glob\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "AUTO = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKxt7SMwpTdE"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMor-1juqQz6"
      },
      "source": [
        "GCS_PATTERN = \"gs://--private-url--/vggface2sub_train_cropped_masked/*/*.jpg\"\n",
        "GCS_OUTPUT = \"gs://--private-url--/tfrecords/vggface2sub_masked_\"\n",
        "SHARDS = 32\n",
        "TARGET_SIZE = [224, 224]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdjIpCkgqen6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "35edb7fd-3872-4a42-c45e-e17df1b7a3e3"
      },
      "source": [
        "total_images = len(glob(\"data/*/*.jpg\"))\n",
        "shard_size = math.ceil(1.0 * total_images / SHARDS)\n",
        "print(\"Processing {0} images into {1} .tfrec files with {2} images each.\".format(total_images, SHARDS, shard_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing 561306 images into 32 .tfrec files with 17541 images each.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHUjSfhm4uf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd3925a8-179d-452d-ac28-eb9cf769ea72"
      },
      "source": [
        "images_files = glob(\"data/*/*.jpg\")\n",
        "random.shuffle(images_files)\n",
        "\n",
        "images_files_batches = [images_files[i * shard_size:(i + 1) * shard_size] for i in range((len(images_files) + shard_size - 1) // shard_size)]\n",
        "\n",
        "print(\"Batches: {0}\".format(len(images_files_batches)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batches: 32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXk-2TLoJcdR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1fb13b54-b193-4d7d-a703-820a56f41b64"
      },
      "source": [
        "labels = glob(\"data/*\")\n",
        "labels = [x.split(\"/\")[-1] for x in labels]\n",
        "\n",
        "print(labels[0:10])\n",
        "np.save(\"labels.npy\", np.asarray(labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['n003459', 'n008150', 'n009195', 'n007356', 'n002760', 'n002377', 'n000014', 'n007034', 'n007702', 'n000899']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aI9cXoq7Hy_"
      },
      "source": [
        "def to_tfrecord(img, label):\n",
        "  str_label = label.decode(\"utf-8\")\n",
        "  class_num = np.argmax(np.array(labels) == str_label)\n",
        "  one_hot = np.eye(len(labels))[class_num].tolist()\n",
        "  \n",
        "  feature = {\n",
        "      \"img\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[img])),\n",
        "      \"label\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[label])),\n",
        "      \"onehot\": tf.train.Feature(float_list=tf.train.FloatList(value=one_hot))\n",
        "  }\n",
        "  return tf.train.Example(features=tf.train.Features(feature=feature))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-_G1MaN7M7Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c9bc0375-cc1a-4f7f-d67e-8b4af9aa63b4"
      },
      "source": [
        "print(\"Writing TFRecords to Google Cloud Storage...\")\n",
        "\n",
        "for batch_no in range(len(images_files_batches)): #batch_no, batch_files in enumerate(images_files_batches):\n",
        "  batch_files = images_files_batches[batch_no]\n",
        "\n",
        "  print(\"Writing batch no {0}\".format(batch_no + 1))\n",
        "\n",
        "  filename = GCS_OUTPUT + \"{:02d}.tfrec\".format(batch_no)\n",
        "  starting = time.time()\n",
        "\n",
        "  with tf.io.TFRecordWriter(filename) as out_file:\n",
        "    for file in tqdm(batch_files):\n",
        "      label = file.split(\"/\")[-2]\n",
        "      label = bytes(label, 'utf-8')\n",
        "\n",
        "      with open(file, \"rb\") as local_file:\n",
        "        bits = local_file.read()\n",
        "\n",
        "      img = tf.image.decode_jpeg(bits)\n",
        "      img = tf.image.resize(img, TARGET_SIZE)\n",
        "      img = tf.cast(img, tf.uint8)\n",
        "      img = tf.image.encode_jpeg(img, optimize_size=True, chroma_downsampling=False)\n",
        "\n",
        "      record = to_tfrecord(img.numpy(), label)\n",
        "      out_file.write(record.SerializeToString())\n",
        "    \n",
        "  delta = time.time() - starting\n",
        "\n",
        "  print(\" >> Batch {0} complete in {1}s!\".format(batch_no + 1, delta))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 23/17541 [00:00<01:23, 210.56it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Writing TFRecords to Google Cloud Storage...\n",
            "Writing batch no 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:17<00:00, 225.92it/s]\n",
            "  0%|          | 24/17541 [00:00<01:13, 239.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 1 complete in 87.77685761451721s!\n",
            "Writing batch no 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:18<00:00, 222.88it/s]\n",
            "  0%|          | 20/17541 [00:00<01:28, 198.77it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 2 complete in 88.48204469680786s!\n",
            "Writing batch no 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:20<00:00, 217.06it/s]\n",
            "  0%|          | 22/17541 [00:00<01:21, 213.97it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 3 complete in 90.15770292282104s!\n",
            "Writing batch no 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:22<00:00, 212.20it/s]\n",
            "  0%|          | 21/17541 [00:00<01:36, 182.29it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 4 complete in 92.12684154510498s!\n",
            "Writing batch no 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:17<00:00, 226.90it/s]\n",
            "  0%|          | 22/17541 [00:00<01:19, 219.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 5 complete in 87.2783784866333s!\n",
            "Writing batch no 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:16<00:00, 229.26it/s]\n",
            "  0%|          | 25/17541 [00:00<01:12, 242.86it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 6 complete in 85.91506767272949s!\n",
            "Writing batch no 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:16<00:00, 228.91it/s]\n",
            "  0%|          | 19/17541 [00:00<01:35, 184.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 7 complete in 86.75346803665161s!\n",
            "Writing batch no 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:19<00:00, 220.37it/s]\n",
            "  0%|          | 25/17541 [00:00<01:10, 247.35it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 8 complete in 89.53097486495972s!\n",
            "Writing batch no 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:18<00:00, 224.29it/s]\n",
            "  0%|          | 23/17541 [00:00<01:16, 229.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 9 complete in 88.33728265762329s!\n",
            "Writing batch no 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:16<00:00, 228.96it/s]\n",
            "  0%|          | 23/17541 [00:00<01:16, 229.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 10 complete in 86.54008054733276s!\n",
            "Writing batch no 11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:16<00:00, 228.05it/s]\n",
            "  0%|          | 23/17541 [00:00<01:16, 228.16it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 11 complete in 86.94231939315796s!\n",
            "Writing batch no 12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:17<00:00, 227.47it/s]\n",
            "  0%|          | 23/17541 [00:00<01:16, 227.96it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 12 complete in 86.92574524879456s!\n",
            "Writing batch no 13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:18<00:00, 223.00it/s]\n",
            "  0%|          | 22/17541 [00:00<01:19, 219.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 13 complete in 88.52751159667969s!\n",
            "Writing batch no 14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:17<00:00, 226.26it/s]\n",
            "  0%|          | 23/17541 [00:00<01:17, 224.99it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 14 complete in 87.49604964256287s!\n",
            "Writing batch no 15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:17<00:00, 226.68it/s]\n",
            "  0%|          | 24/17541 [00:00<01:13, 237.26it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 15 complete in 87.20063924789429s!\n",
            "Writing batch no 16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:16<00:00, 228.21it/s]\n",
            "  0%|          | 24/17541 [00:00<01:14, 234.76it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 16 complete in 87.03564286231995s!\n",
            "Writing batch no 17\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:18<00:00, 224.38it/s]\n",
            "  0%|          | 22/17541 [00:00<01:22, 211.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 17 complete in 88.05421805381775s!\n",
            "Writing batch no 18\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:20<00:00, 218.68it/s]\n",
            "  0%|          | 23/17541 [00:00<01:19, 221.55it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 18 complete in 90.1061155796051s!\n",
            "Writing batch no 19\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:18<00:00, 222.12it/s]\n",
            "  0%|          | 25/17541 [00:00<01:10, 246.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 19 complete in 89.27714228630066s!\n",
            "Writing batch no 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:16<00:00, 227.95it/s]\n",
            "  0%|          | 22/17541 [00:00<01:21, 215.18it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 20 complete in 86.88122367858887s!\n",
            "Writing batch no 21\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:17<00:00, 227.34it/s]\n",
            "  0%|          | 22/17541 [00:00<01:20, 218.24it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 21 complete in 86.43573236465454s!\n",
            "Writing batch no 22\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:18<00:00, 222.20it/s]\n",
            "  0%|          | 20/17541 [00:00<01:29, 196.29it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 22 complete in 88.9230089187622s!\n",
            "Writing batch no 23\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:28<00:00, 198.04it/s]\n",
            "  0%|          | 23/17541 [00:00<01:17, 224.91it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 23 complete in 98.40546584129333s!\n",
            "Writing batch no 24\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:19<00:00, 219.73it/s]\n",
            "  0%|          | 18/17541 [00:00<01:37, 179.75it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 24 complete in 89.8019232749939s!\n",
            "Writing batch no 25\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:18<00:00, 222.97it/s]\n",
            "  0%|          | 23/17541 [00:00<01:17, 225.07it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 25 complete in 88.56018328666687s!\n",
            "Writing batch no 26\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:18<00:00, 223.18it/s]\n",
            "  0%|          | 22/17541 [00:00<01:22, 211.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 26 complete in 88.41051650047302s!\n",
            "Writing batch no 27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:20<00:00, 217.46it/s]\n",
            "  0%|          | 23/17541 [00:00<01:17, 225.35it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 27 complete in 90.65263986587524s!\n",
            "Writing batch no 28\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:20<00:00, 218.96it/s]\n",
            "  0%|          | 23/17541 [00:00<01:16, 229.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 28 complete in 90.45940923690796s!\n",
            "Writing batch no 29\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:18<00:00, 224.32it/s]\n",
            "  0%|          | 20/17541 [00:00<01:28, 198.53it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 29 complete in 88.02453780174255s!\n",
            "Writing batch no 30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:19<00:00, 221.25it/s]\n",
            "  0%|          | 22/17541 [00:00<01:22, 211.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 30 complete in 88.4550392627716s!\n",
            "Writing batch no 31\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17541/17541 [01:19<00:00, 221.87it/s]\n",
            "  0%|          | 21/17535 [00:00<01:23, 209.99it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 31 complete in 89.13739562034607s!\n",
            "Writing batch no 32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 17535/17535 [01:21<00:00, 216.41it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " >> Batch 32 complete in 90.7984230518341s!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzgqLwXRtwCr"
      },
      "source": [
        "def get_data(filename):\n",
        "  img = tf.image.decode_jpeg(tf.io.read_file(filename))\n",
        "  img = tf.image.resize(img, TARGET_SIZE)\n",
        "  img = tf.cast(img, tf.float32) / 255.0;\n",
        "  label = tf.strings.split(tf.expand_dims(filename, axis=-1), sep='/')\n",
        "  label = label.values[-2]\n",
        "\n",
        "  return img, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpJiPm9XwRXf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "443edc48-f489-4dbe-e58f-70b7ee9e0785"
      },
      "source": [
        "def _bytestring_feature(list_of_bytestrings):\n",
        "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))\n",
        "\n",
        "def _int_feature(list_of_ints): # int64\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))\n",
        "\n",
        "def _float_feature(list_of_floats): # float32\n",
        "  return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_floats))\n",
        "\n",
        "def to_tfrecord(tfrec_filewriter, img_bytes, label): \n",
        "  feature = {\n",
        "      \"image\": _bytestring_feature([img_bytes]), # one image in the list\n",
        "      # additional (not very useful) fields to demonstrate TFRecord writing/reading of different types of data\n",
        "      \"label\":         _bytestring_feature([label]),          # fixed length (1) list of strings, the text label\n",
        "  }\n",
        "  return tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "\n",
        "print(\"Getting all images for TFRecord generation\")\n",
        "filenames = tf.data.Dataset.list_files(GCS_PATTERN, seed=520732)\n",
        "dataset1 = filenames.map(get_data, num_parallel_calls=AUTO)\n",
        "dataset1 = dataset1.batch(shard_size)\n",
        "\n",
        "print(\"Writing TFRecord...\")\n",
        "\n",
        "for shard_index, (img, label) in enumerate(dataset1):\n",
        "  filename = GCS_OUTPUT + \"{:02d}-{}.tfrec\".format(shard, shard_size)\n",
        "\n",
        "  with tf.io.TFRecordWriter(filename) as out_file:\n",
        "    for i in range(shard_size):\n",
        "      example = to_tfrecord(out_file, img, label)\n",
        "      out_file.write(example.SerializeToString())\n",
        "    \n",
        "    print(\"Created file {0} with {1} records!\".format(filename, shard_size))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Getting all images for TFRecord generation\n",
            "Writing TFRecord...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}